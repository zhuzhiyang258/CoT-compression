#!/usr/bin/env python3
"""
å¿«é€ŸLoRAåˆå¹¶è„šæœ¬ - ç”¨äºæµ‹è¯•å’ŒéªŒè¯
"""

import os
import argparse
import logging
from pathlib import Path

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def quick_merge(lora_path: str, output_path: str = None):
    """å¿«é€Ÿåˆå¹¶LoRAæƒé‡"""
    
    if output_path is None:
        output_path = f"{lora_path}_merged"
    
    logger.info(f"ğŸ”— å¿«é€ŸLoRAåˆå¹¶")
    logger.info(f"LoRAè·¯å¾„: {lora_path}")
    logger.info(f"è¾“å‡ºè·¯å¾„: {output_path}")
    
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        from peft import PeftModel, PeftConfig
        import torch
        
        # æ£€æŸ¥LoRAé€‚é…å™¨
        if not os.path.exists(lora_path):
            raise FileNotFoundError(f"LoRAé€‚é…å™¨è·¯å¾„ä¸å­˜åœ¨: {lora_path}")
        
        adapter_config_path = os.path.join(lora_path, "adapter_config.json")
        if not os.path.exists(adapter_config_path):
            raise FileNotFoundError(f"adapter_config.jsonä¸å­˜åœ¨: {adapter_config_path}")
        
        # è¯»å–é…ç½®è·å–åŸºç¡€æ¨¡å‹è·¯å¾„
        peft_config = PeftConfig.from_pretrained(lora_path)
        base_model_path = peft_config.base_model_name_or_path
        
        # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œå°è¯•æœ¬åœ°è·¯å¾„
        if not os.path.exists(base_model_path):
            local_base_path = "./models/Qwen3-4B-Chat"
            if os.path.exists(local_base_path):
                base_model_path = local_base_path
                logger.info(f"ä½¿ç”¨æœ¬åœ°åŸºç¡€æ¨¡å‹: {base_model_path}")
            else:
                raise FileNotFoundError(f"åŸºç¡€æ¨¡å‹æœªæ‰¾åˆ°: {base_model_path}")
        
        logger.info(f"åŸºç¡€æ¨¡å‹: {base_model_path}")
        
        # åŠ è½½åŸºç¡€æ¨¡å‹
        logger.info("åŠ è½½åŸºç¡€æ¨¡å‹...")
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True,
            low_cpu_mem_usage=True,
        )
        
        # åŠ è½½LoRAæ¨¡å‹
        logger.info("åŠ è½½LoRAé€‚é…å™¨...")
        model = PeftModel.from_pretrained(
            base_model,
            lora_path,
            torch_dtype=torch.bfloat16,
        )
        
        # åˆå¹¶æƒé‡
        logger.info("åˆå¹¶æƒé‡...")
        merged_model = model.merge_and_unload()
        
        # ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
        logger.info("ä¿å­˜åˆå¹¶åçš„æ¨¡å‹...")
        os.makedirs(output_path, exist_ok=True)
        merged_model.save_pretrained(
            output_path,
            safe_serialization=True,
            max_shard_size="5GB"
        )
        
        # ä¿å­˜tokenizer
        logger.info("ä¿å­˜tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            base_model_path,
            trust_remote_code=True
        )
        tokenizer.save_pretrained(output_path)
        
        logger.info("âœ… åˆå¹¶å®Œæˆ!")
        
        # ç®€å•æµ‹è¯•
        logger.info("ğŸ§ª å¿«é€Ÿæµ‹è¯•...")
        test_input = "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚"
        inputs = tokenizer(test_input, return_tensors="pt")
        inputs = {k: v.to(merged_model.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = merged_model.generate(
                **inputs,
                max_new_tokens=30,
                do_sample=True,
                temperature=0.7,
                pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        logger.info(f"æµ‹è¯•è¾“å‡º: {response}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ åˆå¹¶å¤±è´¥: {e}")
        return False

def main():
    parser = argparse.ArgumentParser(description="å¿«é€ŸLoRAåˆå¹¶")
    parser.add_argument("lora_path", help="LoRAé€‚é…å™¨è·¯å¾„")
    parser.add_argument("-o", "--output", help="è¾“å‡ºè·¯å¾„ (é»˜è®¤: {lora_path}_merged)")
    
    args = parser.parse_args()
    
    success = quick_merge(args.lora_path, args.output)
    return 0 if success else 1

if __name__ == "__main__":
    exit(main())